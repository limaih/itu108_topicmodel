{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run this command. This will clone/download the necessary jupyter notebook and data files required\n",
    "\"\"\"\n",
    "\n",
    "!git clone https://github.com/limaih/itu108_topicmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' step 1. import necessary libraries\n",
    "'''\n",
    "import nltk\n",
    "nltk.download('stopwords') \n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "import string\n",
    "from pathlib import Path\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' step 2. read in files (from directory) for analysis\n",
    "'''\n",
    "#r is the raw sting literals so that windows path slash won't create problem\n",
    "data_folder = Path(r'news') \n",
    "#read each file from the directory into an array and name it corpus \n",
    "corpus = []\n",
    "filenames = []\n",
    "\n",
    "for filename in data_folder.iterdir():\n",
    "    #encoding for macbook: encoding = \"ISO-8859-1\"\n",
    "    fp = open(str(filename), 'r', encoding = \"ISO-8859-1\")\n",
    "    corpus.append(fp.read())\n",
    "    #keep the filename for later use\n",
    "    filenames.append(filename.name) \n",
    "    fp.close()\n",
    "    \n",
    "print(corpus.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' step3. conduct preprocessing steps\n",
    "'''\n",
    "#stemming - English\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "#addon to stop words\n",
    "domain_stop = []\n",
    "stop.update(domain_stop)\n",
    "\n",
    "def clean(doc):\n",
    "    punc_free = ''.join([ch for ch in doc.lower() if ch not in exclude])\n",
    "    stop_free = ' '.join([i for i in punc_free.split() if i not in stop]) \n",
    "    normalized = ' '.join(lemma.lemmatize(word) for word in stop_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' step 4. prepare word representation - term frequency or doc term matrix \n",
    "'''\n",
    "dictionary = corpora.Dictionary(doc_clean) \n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' step 5. create lda model\n",
    "'''\n",
    "topic_num = \n",
    "word_num = \n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics = topic_num, id2word = dictionary, passes=20) \n",
    "\n",
    "pprint(ldamodel.print_topics(num_topics=topic_num, num_words=word_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' step 6. Compute Perplexity\n",
    "'''\n",
    "print('Perplexity: ', )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' step 7. Assigned Topic and Probability\n",
    "'''\n",
    "print('\\nFile name and its corresponding topic id with probability:')\n",
    "dic_topic_doc = {}\n",
    "for index, doc in enumerate(doc_clean):\n",
    "    #for doc in doc_clean:\n",
    "    bow = dictionary.doc2bow(doc)\n",
    "    \n",
    "    #get topic distribution of the ldamodel\n",
    "    t = ldamodel.get_document_topics(bow) \n",
    "   \n",
    "    #sort the probability value in descending order to extract the top contributing topic id \n",
    "    sorted_t = sorted(t, key=lambda x: x[1], reverse=True) \n",
    "    \n",
    "    #print only the filename \n",
    "    print(filenames[index],sorted_t) \n",
    "    \n",
    "    #get the top scoring item \n",
    "    top_item = sorted_t.pop(0) \n",
    "   \n",
    "    #create dictionary and keep key as topic id and filename and probability in tuple as value \n",
    "    dic_topic_doc.setdefault(top_item[0],[]).append((filenames[index],top_item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nTopic id, number of documents, list of documents with probability and represented topic words:')\n",
    "for key,value in dic_topic_doc.items():\n",
    "    sorted_value = sorted(value, key=lambda x: x[1], reverse=True)\n",
    "    print(key,len(value),sorted_value)\n",
    "    #print the topic word and most represented doc\n",
    "    print(ldamodel.print_topic(key,word_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install pyLDAvis package\n",
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' step 8. Visualize topics and keywords\n",
    "'''\n",
    "# plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# visualize the topics and keywords\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(ldamodel, doc_term_matrix, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
